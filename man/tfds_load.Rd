% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/load.R
\name{tfds_load}
\alias{tfds_load}
\title{Loads the named dataset into a TensorFlow Dataset}
\usage{
tfds_load(name, split = NULL, data_dir = NULL, batch_size = NULL,
  in_memory = NULL, download = TRUE, as_supervised = FALSE,
  decoders = NULL, with_info = FALSE, builder_kwargs = NULL,
  download_and_prepare_kwargs = NULL, as_dataset_kwargs = NULL,
  try_gcs = FALSE)
}
\arguments{
\item{name}{`str`, the registered name of the `DatasetBuilder` (the snake case
                                                          version of the class name). This can be either `"dataset_name"` or
`"dataset_name/config_name"` for datasets with `BuilderConfig`s.
As a convenience, this string may contain comma-separated keyword
arguments for the builder. For example `"foo_bar/a=True,b=3"` would use
the `FooBar` dataset passing the keyword arguments `a=True` and `b=3`
(for builders with configs, it would be `"foo_bar/zoo/a=True,b=3"` to
  use the `"zoo"` config and pass to the builder keyword arguments `a=True`
  and `b=3`).}

\item{split}{`tfds.Split` or `str`, which split of the data to load. If None,
will return a `dict` with all splits (typically `tfds.Split.TRAIN` and
                                      `tfds.Split.TEST`).}

\item{data_dir}{`str` (optional), directory to read/write data.
Defaults to "~/tensorflow_datasets".}

\item{batch_size}{`int`, if set, add a batch dimension to examples. Note that
variable length features will be 0-padded. If
`batch_size=-1`, will return the full dataset as `tf.Tensor`s.}

\item{in_memory}{`bool`, if `True`, loads the dataset in memory which
increases iteration speeds. Note that if `True` and the dataset has
unknown dimensions, the features will be padded to the maximum
size across the dataset.}

\item{download}{`bool` (optional), whether to call
`tfds.core.DatasetBuilder.download_and_prepare`
before calling `tf.DatasetBuilder.as_dataset`. If `False`, data is
expected to be in `data_dir`. If `True` and the data is already in
`data_dir`, `download_and_prepare` is a no-op.}

\item{as_supervised}{`bool`, if `True`, the returned `tf.data.Dataset`
will have a 2-tuple structure `(input, label)` according to
`builder.info.supervised_keys`. If `False`, the default,
the returned `tf.data.Dataset` will have a dictionary with all the
features.}

\item{decoders}{Nested dict of `Decoder` objects which allow to customize the
decoding. The structure should match the feature structure, but only
customized feature keys need to be present. See
[the guide](https://github.com/tensorflow/datasets/tree/master/docs/decode.md)
for more info.}

\item{with_info}{`bool`, if True, tfds.load will return the tuple
(tf.data.Dataset, tfds.core.DatasetInfo) containing the info associated
with the builder.}

\item{builder_kwargs}{`dict` (optional), keyword arguments to be passed to the
`tfds.core.DatasetBuilder` constructor. `data_dir` will be passed
through by default.}

\item{download_and_prepare_kwargs}{`dict` (optional) keyword arguments passed to
`tfds.core.DatasetBuilder.download_and_prepare` if `download=True`. Allow
to control where to download and extract the cached data. If not set,
cache_dir and manual_dir will automatically be deduced from data_dir.}

\item{as_dataset_kwargs}{`dict` (optional), keyword arguments passed to
`tfds.core.DatasetBuilder.as_dataset`.}

\item{try_gcs}{`bool`, if True, tfds.load will see if the dataset exists on
the public GCS bucket before building it locally.}
}
\description{
Loads the named dataset into a TensorFlow Dataset
}
